{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, RidgeCV, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_integer_dtype\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from scipy.special import expit\n",
    "import seaborn as sns\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore');\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fully_observed_bandit():\n",
    "    \"\"\"\n",
    "    This loads in a multiclass classification problem and reformulates it as a fully observed bandit problem.\n",
    "    \n",
    "    \"\"\"\n",
    "    df_l = pd.read_csv('data/letter-recognition.data',\n",
    "                       names = ['a']+[f'x{i}' for i in range(16)])\n",
    "    X = df_l.drop(columns=['a'])\n",
    "\n",
    "    # Convert labels to ints and one-hot\n",
    "    y = df_l['a']\n",
    "    # if y is not column of integers (that represent classes), then convert\n",
    "    if not is_integer_dtype(y.dtype):\n",
    "        y = y.astype('category').cat.codes\n",
    "\n",
    "    ## Full rewards\n",
    "    n = len(y)\n",
    "    k = max(y)+1\n",
    "    full_rewards = np.zeros([n, k])\n",
    "    full_rewards[np.arange(0,n),y] = 1\n",
    "    contexts = X\n",
    "    best_actions = y\n",
    "    return contexts, full_rewards, best_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 26 actions, the context space is 16 dimensional, and there are 20000 examples.\n",
      "For example, the first item has context vector:\n",
      "   x0  x1  x2  x3  x4  x5  x6  x7  x8  x9  x10  x11  x12  x13  x14  x15\n",
      "0   2   8   3   5   1   8  13   0   6   6   10    8    0    8    0    8.\n",
      "The best action is 19.  The reward for that action is 1 and all other actions get reward 0.\n",
      "The reward information is store in full_rewards as the row\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0.].\n"
     ]
    }
   ],
   "source": [
    "contexts, full_rewards, best_actions = get_fully_observed_bandit()\n",
    "n, k = full_rewards.shape\n",
    "_, d = contexts.shape\n",
    "print(f\"There are {k} actions, the context space is {d} dimensional, and there are {n} examples.\")\n",
    "print(f\"For example, the first item has context vector:\\n{contexts.iloc[0:1]}.\")\n",
    "print(f\"The best action is {best_actions[0]}.  The reward for that action is 1 and all other actions get reward 0.\")\n",
    "print(f\"The reward information is store in full_rewards as the row\\n{full_rewards[0]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose train/test indices\n",
    "rng = default_rng(7)\n",
    "train_frac = 0.5\n",
    "train_size = round(train_frac * n)\n",
    "train_idx = rng.choice(n, size = train_size, replace = False)\n",
    "test_idx = np.setdiff1d(np.arange(n), train_idx, assume_unique=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self, num_actions=2):\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_action_distribution(self, X):\n",
    "        \"\"\"   \n",
    "        This method is intended to be overridden by each implementation of Policy.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): contexts\n",
    "\n",
    "        Returns:\n",
    "            2-dim numpy array with the same number of rows as X and self.num_actions columns. \n",
    "                Each rows gives the policy's probability distribution over actions conditioned on the context in the corresponding row of X\n",
    "        \"\"\"   \n",
    "        raise NotImplementedError(\"Must override method\")\n",
    "\n",
    "    def get_action_propensities(self, X, actions):\n",
    "        \"\"\"   \n",
    "        Args:\n",
    "            X (pd.DataFrame): contexts, rows correspond to entries of actions\n",
    "            actions (np.array): actions taken, represented by integers, corresponding to rows of X\n",
    "\n",
    "        Returns:\n",
    "            1-dim numpy array of probabilities (same size as actions) for taking each action in its corresponding context\n",
    "        \"\"\"   \n",
    "        ## DONE\n",
    "        action_distribution = self.get_action_distribution(X)\n",
    "        return np.take_along_axis(action_distribution, actions.reshape(-1, 1), axis=1).flatten()\n",
    "\n",
    "    def select_actions(self, X, rng=default_rng(1)):\n",
    "        \"\"\"   \n",
    "        Args:\n",
    "            X (pd.DataFrame): contexts, rows correspond to entries of actions and propensities returned\n",
    "\n",
    "        Returns:\n",
    "            actions (np.array): 1-dim numpy array of length equal to the number of rows of X.  Each entry is an integer indicating the action selected for the corresponding context in X. \n",
    "                The action is selected randomly according to the policy, conditional on the context specified in the appropriate row of X.\n",
    "            propensities (np.array): 1-dim numpy array of length equal to the number of rows of X; gives the propensity for each action selected in actions\n",
    "\n",
    "        \"\"\"   \n",
    "        ## DONE\n",
    "        action_distribution = self.get_action_distribution(X)\n",
    "        actions = np.array([np.random.choice(26, 1, p=action_distribution[i]) for i in range(X.shape[0])]).flatten()\n",
    "        propensities = self.get_action_propensities(X, actions)\n",
    "        assert len(actions) == len(propensities) == X.shape[0]\n",
    "        \n",
    "        return actions, propensities\n",
    "        \n",
    "        \n",
    "    def get_value_estimate(self, X, full_rewards):\n",
    "        \"\"\"   \n",
    "        Args:\n",
    "            X (pd.DataFrame): contexts, rows correspond to entries of full_rewards\n",
    "            full_rewards (np.array): 2-dim numpy array with the same number of rows as X and self.num_actions columns; \n",
    "                each row gives the rewards that would be received for each action for the context in the corresponding row of X.\n",
    "                This would only be known in a full-feedback bandit, or estimated in a direct method\n",
    "\n",
    "        Returns:\n",
    "            scalar value giving the expected average reward received for playing the policy for contexts X and the given full_rewards\n",
    "\n",
    "        \"\"\"   \n",
    "        ## DONE\n",
    "        n = X.shape[0]\n",
    "        actions, propensities = self.select_actions(X)\n",
    "        action_distribution = self.get_action_distribution(X)\n",
    "        \n",
    "        return (full_rewards*action_distribution).sum()/n\n",
    "\n",
    "\n",
    "class UniformActionPolicy(Policy):\n",
    "    def __init__(self, num_actions=2):\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def get_action_distribution(self, X):\n",
    "        ## DONE\n",
    "        return np.full((X.shape[0], self.num_actions), 1.0/self.num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = contexts.iloc[train_idx].to_numpy()\n",
    "y_train = best_actions.iloc[train_idx].to_numpy()\n",
    "X_test = contexts.iloc[test_idx].to_numpy()\n",
    "y_test = best_actions.iloc[test_idx].to_numpy()\n",
    "full_rewards_test = full_rewards[test_idx]\n",
    "\n",
    "uniform_policy = UniformActionPolicy(num_actions=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SKLearnPolicy(Policy):\n",
    "    \"\"\" \n",
    "    An SKLearnPolicy uses a scikit learn model to generate an action distribution.  If the SKLearnPolicy is built with is_deterministic=False, \n",
    "    then the predict distribution for a context x should be whatever predict_proba for the model returns.  If is_deterministic=True, then all the probability mass \n",
    "    should be concentrated on whatever predict of the model returns.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, num_actions=2, is_deterministic=False):\n",
    "        self.is_deterministic = is_deterministic\n",
    "        self.num_actions = num_actions\n",
    "        self.model = model\n",
    "\n",
    "    def get_action_distribution(self, X):\n",
    "        ## DONE\n",
    "        if (self.is_deterministic):\n",
    "            predictions = self.model.predict(X)\n",
    "            return np.eye(self.num_actions)[predictions.reshape(-1)] # one hot\n",
    "        else:\n",
    "            return self.model.predict_proba(X)\n",
    "\n",
    "\n",
    "    def select_actions(self, X, rng=default_rng(1)):\n",
    "        ## DONE\n",
    "        if (self.is_deterministic):\n",
    "            actions = self.model.predict(X)\n",
    "            propensities = np.full(len(actions), 1.0)\n",
    "            return actions, propensities\n",
    "        else:\n",
    "            actions, propensities = Policy.select_actions(self, X)\n",
    "            return actions, propensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(multi_class='multinomial')\n",
    "model.fit(X_train, y_train)\n",
    "policy_stochastic = SKLearnPolicy(model=model, num_actions=k, is_deterministic=False)\n",
    "policy_deterministic = SKLearnPolicy(model=model, num_actions=k, is_deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_w = policy_stochastic.get_action_propensities(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_0 = uniform_policy.get_action_propensities(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96096041, 0.44768231, 0.54512254, ..., 0.06543477, 0.95818714,\n",
       "       0.95826883])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03846154, 0.03846154, 0.03846154, ..., 0.03846154, 0.03846154,\n",
       "       0.03846154])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"data/X_train.npy\")\n",
    "y_train = np.load(\"data/y_train.npy\")\n",
    "X_test = np.load(\"data/X_test.npy\")\n",
    "y_test = np.load(\"data/y_test.npy\")\n",
    "full_rewards_test = np.load(\"data/full_rewards_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "assert X_test.shape[0] == y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = torch.LongTensor(data)\n",
    "        self.targets = torch.LongTensor(targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "dataset = MyDataset(X_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 3,  5,  3,  6,  3,  9,  6,  6,  3,  8,  6, 11,  3,  9,  5,  8],\n",
       "         [ 2,  4,  4,  3,  2,  7,  2,  2,  2,  5,  2,  8,  2,  5,  2,  7],\n",
       "         [ 5, 11,  8,  8,  4,  8,  8,  5,  2,  7,  8,  8,  9,  9,  0,  8],\n",
       "         [ 7, 10,  9,  7,  7,  9,  6,  4,  7,  9,  5,  6,  2,  8,  7, 10],\n",
       "         [ 4,  7,  6,  5,  4,  7,  9,  2,  9, 11,  8,  5,  1,  8,  6,  5]]),\n",
       " tensor([16,  0, 22,  1, 25])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions, props = policy_stochastic.select_actions(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16,  0, 22, ...,  4,  3, 16])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94485422, 0.87272023, 0.64129014, ..., 0.83652646, 0.30229926,\n",
       "       0.55424757])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = (actions == y_train).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 0., 0.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3023, 0.5194, 0.0963, 0.6305, 0.2436],\n",
       "        [0.1353, 0.4597, 0.0173, 0.1843, 0.2086],\n",
       "        [0.6548, 0.4161, 0.2754, 0.6642, 0.6489]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(size=(3, 5))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3023, 0.4597, 0.6489])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.gather(1, torch.LongTensor([0,1,4]).unsqueeze(1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
