{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, RidgeCV, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_integer_dtype\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from scipy.special import expit\n",
    "import seaborn as sns\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore');\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fully_observed_bandit():\n",
    "    \"\"\"\n",
    "    This loads in a multiclass classification problem and reformulates it as a fully observed bandit problem.\n",
    "    \n",
    "    \"\"\"\n",
    "    df_l = pd.read_csv('data/letter-recognition.data',\n",
    "                       names = ['a']+[f'x{i}' for i in range(16)])\n",
    "    X = df_l.drop(columns=['a'])\n",
    "\n",
    "    # Convert labels to ints and one-hot\n",
    "    y = df_l['a']\n",
    "    # if y is not column of integers (that represent classes), then convert\n",
    "    if not is_integer_dtype(y.dtype):\n",
    "        y = y.astype('category').cat.codes\n",
    "\n",
    "    ## Full rewards\n",
    "    n = len(y)\n",
    "    k = max(y)+1\n",
    "    full_rewards = np.zeros([n, k])\n",
    "    full_rewards[np.arange(0,n),y] = 1\n",
    "    contexts = X\n",
    "    best_actions = y\n",
    "    return contexts, full_rewards, best_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 26 actions, the context space is 16 dimensional, and there are 20000 examples.\n",
      "For example, the first item has context vector:\n",
      "   x0  x1  x2  x3  x4  x5  x6  x7  x8  x9  x10  x11  x12  x13  x14  x15\n",
      "0   2   8   3   5   1   8  13   0   6   6   10    8    0    8    0    8.\n",
      "The best action is 19.  The reward for that action is 1 and all other actions get reward 0.\n",
      "The reward information is store in full_rewards as the row\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0.].\n"
     ]
    }
   ],
   "source": [
    "contexts, full_rewards, best_actions = get_fully_observed_bandit()\n",
    "n, k = full_rewards.shape\n",
    "_, d = contexts.shape\n",
    "print(f\"There are {k} actions, the context space is {d} dimensional, and there are {n} examples.\")\n",
    "print(f\"For example, the first item has context vector:\\n{contexts.iloc[0:1]}.\")\n",
    "print(f\"The best action is {best_actions[0]}.  The reward for that action is 1 and all other actions get reward 0.\")\n",
    "print(f\"The reward information is store in full_rewards as the row\\n{full_rewards[0]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose train/test indices\n",
    "rng = default_rng(7)\n",
    "train_frac = 0.5\n",
    "train_size = round(train_frac * n)\n",
    "train_idx = rng.choice(n, size = train_size, replace = False)\n",
    "test_idx = np.setdiff1d(np.arange(n), train_idx, assume_unique=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(contexts.iloc[train_idx].to_numpy(), dtype=torch.float, requires_grad=True)\n",
    "# y_train = torch.tensor(best_actions.iloc[train_idx].to_numpy(), requires_grad=True)\n",
    "\n",
    "X_test = torch.tensor(contexts.iloc[test_idx].to_numpy(), dtype=torch.float, requires_grad=True)\n",
    "# y_test = torch.tensor(best_actions.iloc[test_idx].to_numpy(), requires_grad=True)\n",
    "full_rewards_test = torch.tensor(full_rewards[test_idx], dtype=torch.float, requires_grad=True)\n",
    "\n",
    "# R = torch.nn.Embedding.from_pretrained(full_rewards_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, num_actions=2):\n",
    "        super(Policy, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_action_distribution(self, X):\n",
    "        \"\"\"   \n",
    "        This method is intended to be overridden by each implementation of Policy.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): contexts\n",
    "\n",
    "        Returns:\n",
    "            2-dim numpy array with the same number of rows as X and self.num_actions columns. \n",
    "                Each rows gives the policy's probability distribution over actions conditioned on the context in the corresponding row of X\n",
    "        \"\"\"   \n",
    "        raise NotImplementedError(\"Must override method\")\n",
    "\n",
    "    def get_action_propensities(self, X, actions):\n",
    "        \"\"\"   \n",
    "        Args:\n",
    "            X (pd.DataFrame): contexts, rows correspond to entries of actions\n",
    "            actions (np.array): actions taken, represented by integers, corresponding to rows of X\n",
    "\n",
    "        Returns:\n",
    "            1-dim numpy array of probabilities (same size as actions) for taking each action in its corresponding context\n",
    "        \"\"\"   \n",
    "        Pi = self.get_action_distribution(X)\n",
    "        actions_one_hot = F.one_hot(actions.long(), num_classes=k).float()\n",
    "        propensities = torch.matmul(Pi.unsqueeze(1), actions_one_hot.unsqueeze(2)).squeeze()\n",
    "        return propensities\n",
    "    \n",
    "    def select_actions(self, X, rng=default_rng(1)):\n",
    "        \"\"\"   \n",
    "        Args:\n",
    "            X (pd.DataFrame): contexts, rows correspond to entries of actions and propensities returned\n",
    "\n",
    "        Returns:\n",
    "            actions (np.array): 1-dim numpy array of length equal to the number of rows of X.  Each entry is an integer indicating the action selected for the corresponding context in X. \n",
    "                The action is selected randomly according to the policy, conditional on the context specified in the appropriate row of X.\n",
    "            propensities (np.array): 1-dim numpy array of length equal to the number of rows of X; gives the propensity for each action selected in actions\n",
    "\n",
    "        \"\"\"   \n",
    "        ## TODO\n",
    "        Pi = self.get_action_distribution(X)\n",
    "        actions = [np.random.choice(range(self.num_actions), p=Pi_i.detach().numpy()) for Pi_i in Pi]\n",
    "        actions = torch.tensor(actions)\n",
    "        propensities = self.get_action_propensities(X, actions)\n",
    "        return actions, propensities\n",
    "    \n",
    "    def get_value_estimate(self, X, full_rewards):\n",
    "        \"\"\"   \n",
    "        Args:\n",
    "            X (pd.DataFrame): contexts, rows correspond to entries of full_rewards\n",
    "            full_rewards (np.array): 2-dim numpy array with the same number of rows as X and self.num_actions columns; \n",
    "                each row gives the rewards that would be received for each action for the context in the corresponding row of X.\n",
    "                This would only be known in a full-feedback bandit, or estimated in a direct method\n",
    "\n",
    "        Returns:\n",
    "            scalar value giving the expected average reward received for playing the policy for contexts X and the given full_rewards\n",
    "\n",
    "        \"\"\"   \n",
    "        ## TODO\n",
    "        Pi = self.get_action_distribution(X)\n",
    "        value = torch.sum(Pi * full_rewards, axis=1).mean()\n",
    "        return value\n",
    "\n",
    "\n",
    "############################################################ \n",
    "class UniformActionPolicy(Policy):\n",
    "    def __init__(self, num_actions=2):\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def get_action_distribution(self, X):\n",
    "        ## TODO\n",
    "        p = 1 / self.num_actions\n",
    "        Pi = torch.zeros([X.shape[0], self.num_actions]) + p\n",
    "        return Pi\n",
    "    \n",
    "############################################################\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        outputs = self.softmax(outputs)\n",
    "        return outputs\n",
    "\n",
    "class LogisticPolicy(Policy):\n",
    "    def __init__(self, num_actions, num_features):\n",
    "        super(LogisticPolicy, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.model = LogisticRegression(num_features, num_actions)\n",
    "    \n",
    "    def get_action_distribution(self, X):\n",
    "        Pi = self.model(X)\n",
    "        return Pi\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rewards_vector(full_rewards, actions):\n",
    "    actions_one_hot = F.one_hot(actions.long(), num_classes=k).float()\n",
    "    r = torch.matmul(full_rewards.unsqueeze(1), actions_one_hot.unsqueeze(2)).squeeze()\n",
    "    return r\n",
    "\n",
    "def snips_loss(pi_w, pi_0, r):\n",
    "    return - torch.mean(r * pi_w / pi_0) / torch.mean(pi_w / pi_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = X_test\n",
    "\n",
    "# initialize logging policy (Pi_0) and target policy (Pi_w)\n",
    "uniform_policy = UniformActionPolicy(num_actions=k)\n",
    "logistic_policy = LogisticPolicy(num_actions=k, num_features=X_train.shape[1])\n",
    "\n",
    "# print('Weight at the beginning:')\n",
    "# print(logistic_policy.model.linear.weight)\n",
    "# print()\n",
    "a = logistic_policy.model.linear.weight.clone()\n",
    "\n",
    "# initialize optimizer\n",
    "optimizer = torch.optim.SGD(logistic_policy.parameters(), lr=100)\n",
    "\n",
    "# generate actions from logging policy & get Pi_0\n",
    "actions, pi_0 = uniform_policy.select_actions(X)\n",
    "\n",
    "# get Pi_w\n",
    "pi_w = logistic_policy.get_action_propensities(X, actions)\n",
    "\n",
    "# get rewords\n",
    "r = get_rewards_vector(full_rewards_test, actions)\n",
    "\n",
    "# calculate loss\n",
    "loss = snips_loss(pi_w, pi_0, r)\n",
    "\n",
    "# update parameters\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# print('Weight after learning:')\n",
    "# print(logistic_policy.model.linear.weight)\n",
    "# print()\n",
    "b = logistic_policy.model.linear.weight.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1082e-01, -1.1831e-01, -1.0486e-01, -6.6901e-02, -7.9250e-02,\n",
       "         -2.1214e-02, -1.3232e-01, -7.3511e-02, -1.1162e-01, -7.4463e-02,\n",
       "         -1.0026e-01, -6.6528e-02, -8.6235e-02, -9.1721e-02, -4.9232e-02,\n",
       "         -6.8127e-02],\n",
       "        [-3.1838e-01, -3.8305e-01, -3.0887e-01, -2.5358e-01, -1.2402e-01,\n",
       "         -2.3612e-01, -3.9301e-01, -3.9878e-01, -2.6312e-01, -5.5142e-01,\n",
       "         -5.1802e-01, -6.2810e-01, -2.6790e-01, -4.7892e-01,  6.2076e-02,\n",
       "         -3.2784e-01],\n",
       "        [-4.0172e-01, -5.5495e-01, -5.2473e-01, -4.7398e-01, -3.9025e-01,\n",
       "         -4.2861e-01, -6.2749e-01, -3.0568e-01, -4.4382e-01, -7.2296e-01,\n",
       "         -4.8234e-01, -4.8248e-01, -2.1982e-01, -6.2192e-01, -2.9372e-01,\n",
       "         -4.6464e-01],\n",
       "        [-1.5515e-02, -3.1287e-02, -2.0336e-02, -1.5640e-02, -1.1854e-02,\n",
       "         -3.0477e-03, -3.0626e-02,  2.0487e-02, -8.0523e-03, -4.6785e-02,\n",
       "         -2.7551e-02, -3.5777e-02, -8.2001e-03, -2.5211e-02, -1.5406e-02,\n",
       "         -1.3889e-02],\n",
       "        [ 4.0482e-01,  1.7550e+00,  2.1785e-01,  1.1745e+00,  4.7136e-01,\n",
       "          4.9458e-01,  1.8392e+00,  1.0179e+00,  3.6696e+00,  1.8712e+00,\n",
       "          1.8585e+00,  4.5329e+00, -8.8862e-01,  2.1868e+00,  2.6123e+00,\n",
       "          2.3591e+00],\n",
       "        [-2.6301e-01, -4.1142e-01, -2.4051e-01, -1.4905e-01, -2.5630e-01,\n",
       "         -2.1837e-01,  1.3476e+00, -5.9769e-01, -3.9345e-01,  8.7669e-01,\n",
       "          1.8524e-01, -1.3065e+00, -5.8368e-01,  3.1975e-01, -1.1861e-01,\n",
       "         -2.7594e-01],\n",
       "        [-2.2848e-04, -7.6528e-04, -3.9761e-04, -2.3740e-04, -1.8618e-04,\n",
       "         -3.8584e-04, -2.5906e-03, -1.3249e-04, -1.2646e-03, -3.1179e-03,\n",
       "         -1.8400e-03, -3.6572e-04,  2.6558e-04, -1.1793e-03, -5.1193e-04,\n",
       "         -8.2155e-04],\n",
       "        [ 4.1597e-01, -4.5322e-01,  8.9431e-01, -2.9680e-01,  5.1352e-01,\n",
       "          8.9170e-01, -1.0066e+00,  4.5044e-01, -2.6023e+00, -5.7318e-01,\n",
       "         -2.1089e+00, -2.2143e+00,  1.1142e+00, -1.0430e+00, -8.2239e-01,\n",
       "         -6.0189e-01],\n",
       "        [ 2.1927e-02,  3.0410e-01,  2.7154e-02,  2.0780e-01,  3.1874e-02,\n",
       "          3.4847e-01,  3.2862e-01, -4.0650e-02,  3.0113e-01,  5.0439e-01,\n",
       "          2.6511e-01,  3.6002e-01, -5.8093e-02,  3.5720e-01, -8.2923e-03,\n",
       "          3.4370e-01],\n",
       "        [-7.7984e-04,  9.0774e-04,  1.1474e-05,  1.7545e-03, -5.9521e-04,\n",
       "          1.2434e-02, -3.9294e-04,  2.0090e-04,  1.3024e-03,  9.4530e-03,\n",
       "         -2.1183e-03,  3.0536e-03, -1.3126e-03,  8.9844e-04, -3.0919e-03,\n",
       "          3.9984e-03],\n",
       "        [-2.0260e-03, -1.4906e-03, -4.1849e-03, -4.5678e-04, -4.1704e-04,\n",
       "         -7.0037e-03, -1.3507e-02, -6.3609e-03, -8.9189e-03, -1.5426e-02,\n",
       "         -1.1621e-02, -5.4845e-03, -6.8855e-04, -1.3958e-02, -6.6707e-04,\n",
       "         -5.1159e-03],\n",
       "        [ 1.0832e-02,  2.3285e-02,  1.2798e-02,  1.7180e-02,  7.4337e-03,\n",
       "          8.8872e-03,  6.9907e-03,  1.1649e-02,  2.1139e-02,  6.0561e-03,\n",
       "          2.4408e-03,  1.8361e-02,  6.7551e-04,  1.9618e-02,  2.4494e-03,\n",
       "          1.8483e-02],\n",
       "        [-9.8347e-01, -1.6364e+00, -1.1149e+00, -1.1958e+00, -6.8779e-01,\n",
       "         -8.3868e-01, -1.9738e+00, -9.8768e-01, -1.8780e+00, -2.3775e+00,\n",
       "         -1.9196e+00, -2.0951e+00,  7.6565e-03, -2.2510e+00, -9.0204e-01,\n",
       "         -1.3978e+00],\n",
       "        [ 8.5304e-05,  1.5642e-04,  9.8482e-05,  1.2272e-04,  5.6772e-05,\n",
       "          1.3271e-04,  1.3411e-04,  2.2100e-04,  6.9916e-05,  9.4503e-05,\n",
       "          1.0105e-04,  1.2723e-04,  1.1069e-04,  1.7954e-04, -2.4363e-06,\n",
       "          1.5850e-04],\n",
       "        [-7.9416e-03, -3.6440e-02, -3.0137e-02, -4.1216e-02, -4.4396e-03,\n",
       "         -2.5750e-02, -3.2657e-02,  6.9545e-02, -6.7275e-02, -1.0546e-01,\n",
       "         -2.5748e-02, -2.9219e-02,  2.5306e-02, -2.0564e-02,  1.6021e-02,\n",
       "         -1.2357e-02],\n",
       "        [-8.1792e-05, -2.7673e-04, -1.3205e-04, -1.1221e-04, -1.1696e-04,\n",
       "         -7.3954e-05,  6.0990e-05, -3.1114e-05, -4.7247e-04, -2.1100e-05,\n",
       "         -2.0115e-04, -8.6650e-04, -1.2182e-04,  4.4573e-05, -2.0236e-04,\n",
       "         -1.7251e-04],\n",
       "        [-1.4326e-03, -2.6366e-03, -2.3699e-03, -1.8692e-03, -1.4445e-03,\n",
       "         -3.3441e-03, -4.9825e-03, -7.8149e-04, -3.7000e-03, -6.4148e-03,\n",
       "         -3.8211e-03, -4.2756e-03, -1.6838e-03, -3.9171e-03, -2.1420e-04,\n",
       "         -3.5608e-03],\n",
       "        [ 7.6323e-02,  1.9730e-01,  7.4467e-02,  1.3802e-01,  4.8904e-02,\n",
       "          1.7958e-01,  1.2421e-01,  8.5861e-02,  9.4458e-02,  1.4590e-01,\n",
       "          1.2061e-03,  1.0023e-01,  1.6213e-02,  9.9698e-02,  7.4440e-02,\n",
       "          2.2291e-01],\n",
       "        [-3.6352e-02, -6.9818e-02, -5.3952e-02, -6.1267e-02, -3.6385e-02,\n",
       "         -7.1364e-02, -1.1429e-01, -4.2088e-02, -5.7445e-02, -1.4617e-01,\n",
       "         -1.0363e-01, -9.2787e-02, -3.1091e-02, -1.1751e-01,  1.6572e-02,\n",
       "         -8.7302e-02],\n",
       "        [-3.0323e-01, -5.7267e-01, -3.8562e-01, -4.3774e-01, -3.0436e-01,\n",
       "         -3.4541e-01, -3.7719e-01, -3.5226e-01, -3.5469e-01, -4.9026e-01,\n",
       "         -2.8528e-01, -5.6502e-01, -1.9432e-01, -4.7145e-01, -3.1044e-01,\n",
       "         -5.3504e-01],\n",
       "        [ 1.6928e+00,  2.1698e+00,  1.8007e+00,  1.5899e+00,  8.7143e-01,\n",
       "          6.6468e-01,  1.1946e+00,  1.6589e+00,  2.4422e+00,  2.2194e+00,\n",
       "          3.2934e+00,  2.9182e+00,  1.2642e+00,  2.3229e+00, -1.7854e-01,\n",
       "          1.1444e+00],\n",
       "        [-1.1001e-01, -5.8965e-02, -1.4194e-01, -6.4083e-02,  1.3203e-02,\n",
       "         -3.4126e-01, -3.4828e-02, -4.2315e-01, -2.3818e-01, -3.5103e-01,\n",
       "          1.1121e-01, -2.7079e-01, -4.4142e-02, -7.0740e-02, -4.4159e-02,\n",
       "         -1.9899e-01],\n",
       "        [-5.8559e-02, -9.8692e-02, -8.1699e-02, -6.4314e-02, -3.3087e-02,\n",
       "         -7.0279e-02, -9.1187e-02, -4.7312e-02, -1.1787e-01, -1.6169e-01,\n",
       "         -1.1433e-01, -1.3391e-01, -2.8470e-02, -9.0747e-02, -4.1165e-02,\n",
       "         -9.3795e-02],\n",
       "        [-8.9565e-03, -1.9062e-02, -1.2496e-02, -5.8735e-03, -2.6829e-02,\n",
       "          9.4953e-03, -5.6200e-03, -3.7977e-02,  1.9574e-02, -7.4839e-03,\n",
       "         -1.1428e-02, -3.4145e-04, -1.3472e-02, -4.3916e-03,  3.9465e-03,\n",
       "         -4.8346e-03],\n",
       "        [ 1.4775e-04,  1.1148e-04,  1.6348e-04,  2.0544e-04,  1.6289e-05,\n",
       "          4.3243e-05,  2.7239e-05,  4.1392e-04,  3.3887e-04, -9.7901e-06,\n",
       "          1.1176e-04,  3.5444e-04, -1.0282e-06,  6.0290e-04, -1.4655e-05,\n",
       "          6.0469e-04],\n",
       "        [-4.0752e-04, -1.0859e-03, -4.0683e-04, -5.6587e-04, -4.8359e-04,\n",
       "          9.0493e-04, -2.7047e-04, -1.5020e-03,  4.8433e-04,  1.6665e-04,\n",
       "         -6.0831e-04, -1.3076e-03, -7.6768e-04, -1.4941e-03,  8.5245e-04,\n",
       "         -1.2950e-03]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b - a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
