{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, RidgeCV, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_integer_dtype\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from scipy.special import expit\n",
    "import seaborn as sns\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore');\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fully_observed_bandit():\n",
    "    \"\"\"\n",
    "    This loads in a multiclass classification problem and reformulates it as a fully observed bandit problem.\n",
    "    \n",
    "    \"\"\"\n",
    "    df_l = pd.read_csv('data/letter-recognition.data',\n",
    "                       names = ['a']+[f'x{i}' for i in range(16)])\n",
    "    X = df_l.drop(columns=['a'])\n",
    "\n",
    "    # Convert labels to ints and one-hot\n",
    "    y = df_l['a']\n",
    "    # if y is not column of integers (that represent classes), then convert\n",
    "    if not is_integer_dtype(y.dtype):\n",
    "        y = y.astype('category').cat.codes\n",
    "\n",
    "    ## Full rewards\n",
    "    n = len(y)\n",
    "    k = max(y)+1\n",
    "    full_rewards = np.zeros([n, k])\n",
    "    full_rewards[np.arange(0,n),y] = 1\n",
    "    contexts = X\n",
    "    best_actions = y\n",
    "    return contexts, full_rewards, best_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 26 actions, the context space is 16 dimensional, and there are 20000 examples.\n",
      "For example, the first item has context vector:\n",
      "   x0  x1  x2  x3  x4  x5  x6  x7  x8  x9  x10  x11  x12  x13  x14  x15\n",
      "0   2   8   3   5   1   8  13   0   6   6   10    8    0    8    0    8.\n",
      "The best action is 19.  The reward for that action is 1 and all other actions get reward 0.\n",
      "The reward information is store in full_rewards as the row\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0.].\n"
     ]
    }
   ],
   "source": [
    "contexts, full_rewards, best_actions = get_fully_observed_bandit()\n",
    "n, k = full_rewards.shape\n",
    "_, d = contexts.shape\n",
    "print(f\"There are {k} actions, the context space is {d} dimensional, and there are {n} examples.\")\n",
    "print(f\"For example, the first item has context vector:\\n{contexts.iloc[0:1]}.\")\n",
    "print(f\"The best action is {best_actions[0]}.  The reward for that action is 1 and all other actions get reward 0.\")\n",
    "print(f\"The reward information is store in full_rewards as the row\\n{full_rewards[0]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose train/test indices\n",
    "rng = default_rng(7)\n",
    "train_frac = 0.5\n",
    "train_size = round(train_frac * n)\n",
    "train_idx = rng.choice(n, size = train_size, replace = False)\n",
    "test_idx = np.setdiff1d(np.arange(n), train_idx, assume_unique=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(contexts.iloc[train_idx].to_numpy(), dtype=torch.float, requires_grad=True)\n",
    "# y_train = torch.tensor(best_actions.iloc[train_idx].to_numpy(), requires_grad=True)\n",
    "\n",
    "X_test = torch.tensor(contexts.iloc[test_idx].to_numpy(), dtype=torch.float, requires_grad=True)\n",
    "# y_test = torch.tensor(best_actions.iloc[test_idx].to_numpy(), requires_grad=True)\n",
    "full_rewards_test = torch.tensor(full_rewards[test_idx], requires_grad=True)\n",
    "\n",
    "# R = torch.nn.Embedding.from_pretrained(full_rewards_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, num_actions=2):\n",
    "        super(Policy, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_action_distribution(self, X):\n",
    "        \"\"\"   \n",
    "        This method is intended to be overridden by each implementation of Policy.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): contexts\n",
    "\n",
    "        Returns:\n",
    "            2-dim numpy array with the same number of rows as X and self.num_actions columns. \n",
    "                Each rows gives the policy's probability distribution over actions conditioned on the context in the corresponding row of X\n",
    "        \"\"\"   \n",
    "        raise NotImplementedError(\"Must override method\")\n",
    "\n",
    "    def get_action_propensities(self, X, actions):\n",
    "        \"\"\"   \n",
    "        Args:\n",
    "            X (pd.DataFrame): contexts, rows correspond to entries of actions\n",
    "            actions (np.array): actions taken, represented by integers, corresponding to rows of X\n",
    "\n",
    "        Returns:\n",
    "            1-dim numpy array of probabilities (same size as actions) for taking each action in its corresponding context\n",
    "        \"\"\"   \n",
    "        ## TODO\n",
    "        Pi = self.get_action_distribution(X)\n",
    "        propensities = torch.tensor([Pi[i][a] for i, a in enumerate(actions)], requires_grad=True)\n",
    "        return propensities\n",
    "    \n",
    "    def select_actions(self, X, rng=default_rng(1)):\n",
    "        \"\"\"   \n",
    "        Args:\n",
    "            X (pd.DataFrame): contexts, rows correspond to entries of actions and propensities returned\n",
    "\n",
    "        Returns:\n",
    "            actions (np.array): 1-dim numpy array of length equal to the number of rows of X.  Each entry is an integer indicating the action selected for the corresponding context in X. \n",
    "                The action is selected randomly according to the policy, conditional on the context specified in the appropriate row of X.\n",
    "            propensities (np.array): 1-dim numpy array of length equal to the number of rows of X; gives the propensity for each action selected in actions\n",
    "\n",
    "        \"\"\"   \n",
    "        ## TODO\n",
    "        Pi = self.get_action_distribution(X)\n",
    "        actions = [np.random.choice(range(self.num_actions), p=Pi_i.detach().numpy()) for Pi_i in Pi]\n",
    "        actions = torch.tensor(actions)\n",
    "        propensities = self.get_action_propensities(X, actions)\n",
    "        return actions, propensities\n",
    "        \n",
    "    def get_value_estimate(self, X, full_rewards):\n",
    "        \"\"\"   \n",
    "        Args:\n",
    "            X (pd.DataFrame): contexts, rows correspond to entries of full_rewards\n",
    "            full_rewards (np.array): 2-dim numpy array with the same number of rows as X and self.num_actions columns; \n",
    "                each row gives the rewards that would be received for each action for the context in the corresponding row of X.\n",
    "                This would only be known in a full-feedback bandit, or estimated in a direct method\n",
    "\n",
    "        Returns:\n",
    "            scalar value giving the expected average reward received for playing the policy for contexts X and the given full_rewards\n",
    "\n",
    "        \"\"\"   \n",
    "        ## TODO\n",
    "        Pi = self.get_action_distribution(X)\n",
    "        value = torch.sum(Pi * full_rewards, axis=1).mean()\n",
    "        return value\n",
    "\n",
    "\n",
    "############################################################ \n",
    "class UniformActionPolicy(Policy):\n",
    "    def __init__(self, num_actions=2):\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def get_action_distribution(self, X):\n",
    "        ## TODO\n",
    "        p = 1 / self.num_actions\n",
    "        Pi = torch.zeros([X.shape[0], self.num_actions]) + p\n",
    "        return Pi\n",
    "    \n",
    "############################################################\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        outputs = self.softmax(outputs)\n",
    "        return outputs\n",
    "\n",
    "class LogisticPolicy(Policy):\n",
    "    def __init__(self, num_actions, num_features):\n",
    "        super(LogisticPolicy, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.model = LogisticRegression(num_features, num_actions)\n",
    "    \n",
    "    def get_action_distribution(self, X):\n",
    "        Pi = self.model(X)\n",
    "        return Pi\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rewards_vector(full_rewards, actions):\n",
    "    return torch.tensor([full_rewards[i][a]for i, a in enumerate(actions)], requires_grad=True)\n",
    "\n",
    "def snips_loss(pi_w, pi_0, r):\n",
    "    return - torch.mean(r * pi_w / pi_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = X_test\n",
    "\n",
    "# initialize logging policy (Pi_0) and target policy (Pi_w)\n",
    "uniform_policy = UniformActionPolicy(num_actions=k)\n",
    "logistic_policy = LogisticPolicy(num_actions=k, num_features=X_train.shape[1])\n",
    "\n",
    "# print('Weight at the beginning:')\n",
    "# print(logistic_policy.model.linear.weight)\n",
    "# print()\n",
    "a = logistic_policy.model.linear.weight.clone()\n",
    "\n",
    "# initialize optimizer\n",
    "optimizer = torch.optim.SGD(logistic_policy.parameters(), lr=100)\n",
    "\n",
    "# generate actions from logging policy & get Pi_0\n",
    "actions, pi_0 = uniform_policy.select_actions(X)\n",
    "\n",
    "# get Pi_w\n",
    "pi_w = logistic_policy.get_action_propensities(X, actions)\n",
    "\n",
    "# get rewords\n",
    "r = get_rewards_vector(full_rewards_test, actions)\n",
    "\n",
    "# calculate loss\n",
    "loss = snips_loss(pi_w, pi_0, r)\n",
    "\n",
    "# update parameters\n",
    "optimizer.zero_grad()\n",
    "for param in logistic_policy.parameters():\n",
    "    param.retain_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# print('Weight after learning:')\n",
    "# print(logistic_policy.model.linear.weight)\n",
    "# print()\n",
    "b = logistic_policy.model.linear.weight.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0384, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
